{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e737ae3b-1855-40c9-ad3e-acf835c64c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 12:59:36 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2022-02-16 12:59:36 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-02-16 12:59:36 INFO: Use device: cpu\n",
      "2022-02-16 12:59:36 INFO: Loading: tokenize\n",
      "2022-02-16 12:59:36 INFO: Loading: pos\n",
      "2022-02-16 12:59:36 INFO: Loading: lemma\n",
      "2022-02-16 12:59:36 INFO: Loading: depparse\n",
      "2022-02-16 12:59:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import tokenize, word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from spacy.symbols import nsubj, VERB\n",
    "from spacy import displacy\n",
    "import textacy\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_stanford = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94073f02-701b-4074-a163-541a32901dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A language is a structured system of communication used by humans. Languages can be based on speech and gesture, sign, or writing. The structure of language is its grammar and the free components are its vocabulary. Many languages, including the most widely spoken ones, have writing systems that enable sounds or signs to be recorded for later reactivation. Human language is unique among the known systems of animal communication in that it is not dependent on a single mode of transmission, is highly variable between cultures and across time, and affords a much wider range of expression than other systems.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>bigram token previous</th>\n",
       "      <th>bigram token next</th>\n",
       "      <th>trigram token previous</th>\n",
       "      <th>trigram token next</th>\n",
       "      <th>Dependency</th>\n",
       "      <th>bigram dep previous</th>\n",
       "      <th>bigram dep next</th>\n",
       "      <th>trigram dep previous</th>\n",
       "      <th>trigram dep next</th>\n",
       "      <th>Direct Parent</th>\n",
       "      <th>POS Parent</th>\n",
       "      <th>POS Token</th>\n",
       "      <th>bigram pos previous</th>\n",
       "      <th>bigram pos next</th>\n",
       "      <th>trigram pos previous</th>\n",
       "      <th>trigram pos next</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Constituents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>(&lt;s&gt;, a)</td>\n",
       "      <td>(a, language)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, a)</td>\n",
       "      <td>(a, language, is)</td>\n",
       "      <td>det</td>\n",
       "      <td>(&lt;s&gt;, det)</td>\n",
       "      <td>(det, nsubj)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, det)</td>\n",
       "      <td>(det, nsubj, ROOT)</td>\n",
       "      <td>language</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>(&lt;s&gt;, DET)</td>\n",
       "      <td>(DET, NOUN)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, DET)</td>\n",
       "      <td>(DET, NOUN, AUX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[A]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>language</td>\n",
       "      <td>(a, language)</td>\n",
       "      <td>(language, is)</td>\n",
       "      <td>(&lt;s&gt;, a, language)</td>\n",
       "      <td>(language, is, a)</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>(det, nsubj)</td>\n",
       "      <td>(nsubj, ROOT)</td>\n",
       "      <td>(&lt;s&gt;, det, nsubj)</td>\n",
       "      <td>(nsubj, ROOT, det)</td>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(DET, NOUN)</td>\n",
       "      <td>(NOUN, AUX)</td>\n",
       "      <td>(&lt;s&gt;, DET, NOUN)</td>\n",
       "      <td>(NOUN, AUX, DET)</td>\n",
       "      <td>[A]</td>\n",
       "      <td>[A, language]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is</td>\n",
       "      <td>(language, is)</td>\n",
       "      <td>(is, a)</td>\n",
       "      <td>(a, language, is)</td>\n",
       "      <td>(is, a, structured)</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>(nsubj, ROOT)</td>\n",
       "      <td>(ROOT, det)</td>\n",
       "      <td>(det, nsubj, ROOT)</td>\n",
       "      <td>(ROOT, det, amod)</td>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>AUX</td>\n",
       "      <td>(NOUN, AUX)</td>\n",
       "      <td>(AUX, DET)</td>\n",
       "      <td>(DET, NOUN, AUX)</td>\n",
       "      <td>(AUX, DET, ADJ)</td>\n",
       "      <td>[language, system, .]</td>\n",
       "      <td>[A, language, is, a, structured, system, of, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>(is, a)</td>\n",
       "      <td>(a, structured)</td>\n",
       "      <td>(language, is, a)</td>\n",
       "      <td>(a, structured, system)</td>\n",
       "      <td>det</td>\n",
       "      <td>(ROOT, det)</td>\n",
       "      <td>(det, amod)</td>\n",
       "      <td>(nsubj, ROOT, det)</td>\n",
       "      <td>(det, amod, attr)</td>\n",
       "      <td>system</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>DET</td>\n",
       "      <td>(AUX, DET)</td>\n",
       "      <td>(DET, ADJ)</td>\n",
       "      <td>(NOUN, AUX, DET)</td>\n",
       "      <td>(DET, ADJ, NOUN)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>structured</td>\n",
       "      <td>(a, structured)</td>\n",
       "      <td>(structured, system)</td>\n",
       "      <td>(is, a, structured)</td>\n",
       "      <td>(structured, system, of)</td>\n",
       "      <td>amod</td>\n",
       "      <td>(det, amod)</td>\n",
       "      <td>(amod, attr)</td>\n",
       "      <td>(ROOT, det, amod)</td>\n",
       "      <td>(amod, attr, prep)</td>\n",
       "      <td>system</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>(DET, ADJ)</td>\n",
       "      <td>(ADJ, NOUN)</td>\n",
       "      <td>(AUX, DET, ADJ)</td>\n",
       "      <td>(ADJ, NOUN, ADP)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[structured]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>system</td>\n",
       "      <td>(structured, system)</td>\n",
       "      <td>(system, of)</td>\n",
       "      <td>(a, structured, system)</td>\n",
       "      <td>(system, of, communication)</td>\n",
       "      <td>attr</td>\n",
       "      <td>(amod, attr)</td>\n",
       "      <td>(attr, prep)</td>\n",
       "      <td>(det, amod, attr)</td>\n",
       "      <td>(attr, prep, pobj)</td>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(ADJ, NOUN)</td>\n",
       "      <td>(NOUN, ADP)</td>\n",
       "      <td>(DET, ADJ, NOUN)</td>\n",
       "      <td>(NOUN, ADP, NOUN)</td>\n",
       "      <td>[a, structured, of]</td>\n",
       "      <td>[a, structured, system, of, communication, use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>(system, of)</td>\n",
       "      <td>(of, communication)</td>\n",
       "      <td>(structured, system, of)</td>\n",
       "      <td>(of, communication, used)</td>\n",
       "      <td>prep</td>\n",
       "      <td>(attr, prep)</td>\n",
       "      <td>(prep, pobj)</td>\n",
       "      <td>(amod, attr, prep)</td>\n",
       "      <td>(prep, pobj, acl)</td>\n",
       "      <td>system</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP</td>\n",
       "      <td>(NOUN, ADP)</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(ADJ, NOUN, ADP)</td>\n",
       "      <td>(ADP, NOUN, VERB)</td>\n",
       "      <td>[communication]</td>\n",
       "      <td>[of, communication, used, by, humans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>communication</td>\n",
       "      <td>(of, communication)</td>\n",
       "      <td>(communication, used)</td>\n",
       "      <td>(system, of, communication)</td>\n",
       "      <td>(communication, used, by)</td>\n",
       "      <td>pobj</td>\n",
       "      <td>(prep, pobj)</td>\n",
       "      <td>(pobj, acl)</td>\n",
       "      <td>(attr, prep, pobj)</td>\n",
       "      <td>(pobj, acl, agent)</td>\n",
       "      <td>of</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(NOUN, VERB)</td>\n",
       "      <td>(NOUN, ADP, NOUN)</td>\n",
       "      <td>(NOUN, VERB, ADP)</td>\n",
       "      <td>[used]</td>\n",
       "      <td>[communication, used, by, humans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>used</td>\n",
       "      <td>(communication, used)</td>\n",
       "      <td>(used, by)</td>\n",
       "      <td>(of, communication, used)</td>\n",
       "      <td>(used, by, humans)</td>\n",
       "      <td>acl</td>\n",
       "      <td>(pobj, acl)</td>\n",
       "      <td>(acl, agent)</td>\n",
       "      <td>(prep, pobj, acl)</td>\n",
       "      <td>(acl, agent, pobj)</td>\n",
       "      <td>communication</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>VERB</td>\n",
       "      <td>(NOUN, VERB)</td>\n",
       "      <td>(VERB, ADP)</td>\n",
       "      <td>(ADP, NOUN, VERB)</td>\n",
       "      <td>(VERB, ADP, NOUN)</td>\n",
       "      <td>[by]</td>\n",
       "      <td>[used, by, humans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>by</td>\n",
       "      <td>(used, by)</td>\n",
       "      <td>(by, humans)</td>\n",
       "      <td>(communication, used, by)</td>\n",
       "      <td>(by, humans, .)</td>\n",
       "      <td>agent</td>\n",
       "      <td>(acl, agent)</td>\n",
       "      <td>(agent, pobj)</td>\n",
       "      <td>(pobj, acl, agent)</td>\n",
       "      <td>(agent, pobj, punct)</td>\n",
       "      <td>used</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>(VERB, ADP)</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(NOUN, VERB, ADP)</td>\n",
       "      <td>(ADP, NOUN, PUNCT)</td>\n",
       "      <td>[humans]</td>\n",
       "      <td>[by, humans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>humans</td>\n",
       "      <td>(by, humans)</td>\n",
       "      <td>(humans, .)</td>\n",
       "      <td>(used, by, humans)</td>\n",
       "      <td>(humans, ., &lt;/s&gt;)</td>\n",
       "      <td>pobj</td>\n",
       "      <td>(agent, pobj)</td>\n",
       "      <td>(pobj, punct)</td>\n",
       "      <td>(acl, agent, pobj)</td>\n",
       "      <td>(pobj, punct, &lt;/s&gt;)</td>\n",
       "      <td>by</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(NOUN, PUNCT)</td>\n",
       "      <td>(VERB, ADP, NOUN)</td>\n",
       "      <td>(NOUN, PUNCT, &lt;/s&gt;)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[humans]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>(humans, .)</td>\n",
       "      <td>(., &lt;/s&gt;)</td>\n",
       "      <td>(by, humans, .)</td>\n",
       "      <td>(., &lt;/s&gt;, &lt;/s&gt;)</td>\n",
       "      <td>punct</td>\n",
       "      <td>(pobj, punct)</td>\n",
       "      <td>(punct, &lt;/s&gt;)</td>\n",
       "      <td>(agent, pobj, punct)</td>\n",
       "      <td>(punct, &lt;/s&gt;, &lt;/s&gt;)</td>\n",
       "      <td>is</td>\n",
       "      <td>AUX</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>(NOUN, PUNCT)</td>\n",
       "      <td>(PUNCT, &lt;/s&gt;)</td>\n",
       "      <td>(ADP, NOUN, PUNCT)</td>\n",
       "      <td>(PUNCT, &lt;/s&gt;, &lt;/s&gt;)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Languages</td>\n",
       "      <td>(&lt;s&gt;, languages)</td>\n",
       "      <td>(languages, can)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, languages)</td>\n",
       "      <td>(languages, can, be)</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td>(&lt;s&gt;, nsubjpass)</td>\n",
       "      <td>(nsubjpass, aux)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, nsubjpass)</td>\n",
       "      <td>(nsubjpass, aux, auxpass)</td>\n",
       "      <td>based</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(&lt;s&gt;, NOUN)</td>\n",
       "      <td>(NOUN, AUX)</td>\n",
       "      <td>(&lt;s&gt;, &lt;s&gt;, NOUN)</td>\n",
       "      <td>(NOUN, AUX, AUX)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Languages]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>can</td>\n",
       "      <td>(languages, can)</td>\n",
       "      <td>(can, be)</td>\n",
       "      <td>(&lt;s&gt;, languages, can)</td>\n",
       "      <td>(can, be, based)</td>\n",
       "      <td>aux</td>\n",
       "      <td>(nsubjpass, aux)</td>\n",
       "      <td>(aux, auxpass)</td>\n",
       "      <td>(&lt;s&gt;, nsubjpass, aux)</td>\n",
       "      <td>(aux, auxpass, ROOT)</td>\n",
       "      <td>based</td>\n",
       "      <td>VERB</td>\n",
       "      <td>AUX</td>\n",
       "      <td>(NOUN, AUX)</td>\n",
       "      <td>(AUX, AUX)</td>\n",
       "      <td>(&lt;s&gt;, NOUN, AUX)</td>\n",
       "      <td>(AUX, AUX, VERB)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[can]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>be</td>\n",
       "      <td>(can, be)</td>\n",
       "      <td>(be, based)</td>\n",
       "      <td>(languages, can, be)</td>\n",
       "      <td>(be, based, on)</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>(aux, auxpass)</td>\n",
       "      <td>(auxpass, ROOT)</td>\n",
       "      <td>(nsubjpass, aux, auxpass)</td>\n",
       "      <td>(auxpass, ROOT, prep)</td>\n",
       "      <td>based</td>\n",
       "      <td>VERB</td>\n",
       "      <td>AUX</td>\n",
       "      <td>(AUX, AUX)</td>\n",
       "      <td>(AUX, VERB)</td>\n",
       "      <td>(NOUN, AUX, AUX)</td>\n",
       "      <td>(AUX, VERB, ADP)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[be]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>based</td>\n",
       "      <td>(be, based)</td>\n",
       "      <td>(based, on)</td>\n",
       "      <td>(can, be, based)</td>\n",
       "      <td>(based, on, speech)</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>(auxpass, ROOT)</td>\n",
       "      <td>(ROOT, prep)</td>\n",
       "      <td>(aux, auxpass, ROOT)</td>\n",
       "      <td>(ROOT, prep, pobj)</td>\n",
       "      <td>based</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td>(AUX, VERB)</td>\n",
       "      <td>(VERB, ADP)</td>\n",
       "      <td>(AUX, AUX, VERB)</td>\n",
       "      <td>(VERB, ADP, NOUN)</td>\n",
       "      <td>[Languages, can, be, on, .]</td>\n",
       "      <td>[Languages, can, be, based, on, speech, and, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>on</td>\n",
       "      <td>(based, on)</td>\n",
       "      <td>(on, speech)</td>\n",
       "      <td>(be, based, on)</td>\n",
       "      <td>(on, speech, and)</td>\n",
       "      <td>prep</td>\n",
       "      <td>(ROOT, prep)</td>\n",
       "      <td>(prep, pobj)</td>\n",
       "      <td>(auxpass, ROOT, prep)</td>\n",
       "      <td>(prep, pobj, cc)</td>\n",
       "      <td>based</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADP</td>\n",
       "      <td>(VERB, ADP)</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(AUX, VERB, ADP)</td>\n",
       "      <td>(ADP, NOUN, CCONJ)</td>\n",
       "      <td>[speech]</td>\n",
       "      <td>[on, speech, and, gesture, ,, sign, ,, or, wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>speech</td>\n",
       "      <td>(on, speech)</td>\n",
       "      <td>(speech, and)</td>\n",
       "      <td>(based, on, speech)</td>\n",
       "      <td>(speech, and, gesture)</td>\n",
       "      <td>pobj</td>\n",
       "      <td>(prep, pobj)</td>\n",
       "      <td>(pobj, cc)</td>\n",
       "      <td>(ROOT, prep, pobj)</td>\n",
       "      <td>(pobj, cc, conj)</td>\n",
       "      <td>on</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(ADP, NOUN)</td>\n",
       "      <td>(NOUN, CCONJ)</td>\n",
       "      <td>(VERB, ADP, NOUN)</td>\n",
       "      <td>(NOUN, CCONJ, NOUN)</td>\n",
       "      <td>[and, gesture, sign]</td>\n",
       "      <td>[speech, and, gesture, ,, sign, ,, or, writing]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and</td>\n",
       "      <td>(speech, and)</td>\n",
       "      <td>(and, gesture)</td>\n",
       "      <td>(on, speech, and)</td>\n",
       "      <td>(and, gesture, ,)</td>\n",
       "      <td>cc</td>\n",
       "      <td>(pobj, cc)</td>\n",
       "      <td>(cc, conj)</td>\n",
       "      <td>(prep, pobj, cc)</td>\n",
       "      <td>(cc, conj, punct)</td>\n",
       "      <td>speech</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>(NOUN, CCONJ)</td>\n",
       "      <td>(CCONJ, NOUN)</td>\n",
       "      <td>(ADP, NOUN, CCONJ)</td>\n",
       "      <td>(CCONJ, NOUN, PUNCT)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[and]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gesture</td>\n",
       "      <td>(and, gesture)</td>\n",
       "      <td>(gesture, ,)</td>\n",
       "      <td>(speech, and, gesture)</td>\n",
       "      <td>(gesture, ,, sign)</td>\n",
       "      <td>conj</td>\n",
       "      <td>(cc, conj)</td>\n",
       "      <td>(conj, punct)</td>\n",
       "      <td>(pobj, cc, conj)</td>\n",
       "      <td>(conj, punct, conj)</td>\n",
       "      <td>speech</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>(CCONJ, NOUN)</td>\n",
       "      <td>(NOUN, PUNCT)</td>\n",
       "      <td>(NOUN, CCONJ, NOUN)</td>\n",
       "      <td>(NOUN, PUNCT, VERB)</td>\n",
       "      <td>[,]</td>\n",
       "      <td>[gesture, ,]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Tokens  bigram token previous      bigram token next  \\\n",
       "0               A               (<s>, a)          (a, language)   \n",
       "1        language          (a, language)         (language, is)   \n",
       "2              is         (language, is)                (is, a)   \n",
       "3               a                (is, a)        (a, structured)   \n",
       "4      structured        (a, structured)   (structured, system)   \n",
       "5          system   (structured, system)           (system, of)   \n",
       "6              of           (system, of)    (of, communication)   \n",
       "7   communication    (of, communication)  (communication, used)   \n",
       "8            used  (communication, used)             (used, by)   \n",
       "9              by             (used, by)           (by, humans)   \n",
       "10         humans           (by, humans)            (humans, .)   \n",
       "11              .            (humans, .)              (., </s>)   \n",
       "12      Languages       (<s>, languages)       (languages, can)   \n",
       "13            can       (languages, can)              (can, be)   \n",
       "14             be              (can, be)            (be, based)   \n",
       "15          based            (be, based)            (based, on)   \n",
       "16             on            (based, on)           (on, speech)   \n",
       "17         speech           (on, speech)          (speech, and)   \n",
       "18            and          (speech, and)         (and, gesture)   \n",
       "19        gesture         (and, gesture)           (gesture, ,)   \n",
       "\n",
       "         trigram token previous           trigram token next Dependency  \\\n",
       "0                 (<s>, <s>, a)            (a, language, is)        det   \n",
       "1            (<s>, a, language)            (language, is, a)      nsubj   \n",
       "2             (a, language, is)          (is, a, structured)       ROOT   \n",
       "3             (language, is, a)      (a, structured, system)        det   \n",
       "4           (is, a, structured)     (structured, system, of)       amod   \n",
       "5       (a, structured, system)  (system, of, communication)       attr   \n",
       "6      (structured, system, of)    (of, communication, used)       prep   \n",
       "7   (system, of, communication)    (communication, used, by)       pobj   \n",
       "8     (of, communication, used)           (used, by, humans)        acl   \n",
       "9     (communication, used, by)              (by, humans, .)      agent   \n",
       "10           (used, by, humans)            (humans, ., </s>)       pobj   \n",
       "11              (by, humans, .)              (., </s>, </s>)      punct   \n",
       "12        (<s>, <s>, languages)         (languages, can, be)  nsubjpass   \n",
       "13        (<s>, languages, can)             (can, be, based)        aux   \n",
       "14         (languages, can, be)              (be, based, on)    auxpass   \n",
       "15             (can, be, based)          (based, on, speech)       ROOT   \n",
       "16              (be, based, on)            (on, speech, and)       prep   \n",
       "17          (based, on, speech)       (speech, and, gesture)       pobj   \n",
       "18            (on, speech, and)            (and, gesture, ,)         cc   \n",
       "19       (speech, and, gesture)           (gesture, ,, sign)       conj   \n",
       "\n",
       "   bigram dep previous   bigram dep next       trigram dep previous  \\\n",
       "0           (<s>, det)      (det, nsubj)            (<s>, <s>, det)   \n",
       "1         (det, nsubj)     (nsubj, ROOT)          (<s>, det, nsubj)   \n",
       "2        (nsubj, ROOT)       (ROOT, det)         (det, nsubj, ROOT)   \n",
       "3          (ROOT, det)       (det, amod)         (nsubj, ROOT, det)   \n",
       "4          (det, amod)      (amod, attr)          (ROOT, det, amod)   \n",
       "5         (amod, attr)      (attr, prep)          (det, amod, attr)   \n",
       "6         (attr, prep)      (prep, pobj)         (amod, attr, prep)   \n",
       "7         (prep, pobj)       (pobj, acl)         (attr, prep, pobj)   \n",
       "8          (pobj, acl)      (acl, agent)          (prep, pobj, acl)   \n",
       "9         (acl, agent)     (agent, pobj)         (pobj, acl, agent)   \n",
       "10       (agent, pobj)     (pobj, punct)         (acl, agent, pobj)   \n",
       "11       (pobj, punct)     (punct, </s>)       (agent, pobj, punct)   \n",
       "12    (<s>, nsubjpass)  (nsubjpass, aux)      (<s>, <s>, nsubjpass)   \n",
       "13    (nsubjpass, aux)    (aux, auxpass)      (<s>, nsubjpass, aux)   \n",
       "14      (aux, auxpass)   (auxpass, ROOT)  (nsubjpass, aux, auxpass)   \n",
       "15     (auxpass, ROOT)      (ROOT, prep)       (aux, auxpass, ROOT)   \n",
       "16        (ROOT, prep)      (prep, pobj)      (auxpass, ROOT, prep)   \n",
       "17        (prep, pobj)        (pobj, cc)         (ROOT, prep, pobj)   \n",
       "18          (pobj, cc)        (cc, conj)           (prep, pobj, cc)   \n",
       "19          (cc, conj)     (conj, punct)           (pobj, cc, conj)   \n",
       "\n",
       "             trigram dep next  Direct Parent POS Parent POS Token  \\\n",
       "0          (det, nsubj, ROOT)       language       NOUN       DET   \n",
       "1          (nsubj, ROOT, det)             is        AUX      NOUN   \n",
       "2           (ROOT, det, amod)             is        AUX       AUX   \n",
       "3           (det, amod, attr)         system       NOUN       DET   \n",
       "4          (amod, attr, prep)         system       NOUN       ADJ   \n",
       "5          (attr, prep, pobj)             is        AUX      NOUN   \n",
       "6           (prep, pobj, acl)         system       NOUN       ADP   \n",
       "7          (pobj, acl, agent)             of        ADP      NOUN   \n",
       "8          (acl, agent, pobj)  communication       NOUN      VERB   \n",
       "9        (agent, pobj, punct)           used       VERB       ADP   \n",
       "10        (pobj, punct, </s>)             by        ADP      NOUN   \n",
       "11        (punct, </s>, </s>)             is        AUX     PUNCT   \n",
       "12  (nsubjpass, aux, auxpass)          based       VERB      NOUN   \n",
       "13       (aux, auxpass, ROOT)          based       VERB       AUX   \n",
       "14      (auxpass, ROOT, prep)          based       VERB       AUX   \n",
       "15         (ROOT, prep, pobj)          based       VERB      VERB   \n",
       "16           (prep, pobj, cc)          based       VERB       ADP   \n",
       "17           (pobj, cc, conj)             on        ADP      NOUN   \n",
       "18          (cc, conj, punct)         speech       NOUN     CCONJ   \n",
       "19        (conj, punct, conj)         speech       NOUN      NOUN   \n",
       "\n",
       "   bigram pos previous bigram pos next trigram pos previous  \\\n",
       "0           (<s>, DET)     (DET, NOUN)      (<s>, <s>, DET)   \n",
       "1          (DET, NOUN)     (NOUN, AUX)     (<s>, DET, NOUN)   \n",
       "2          (NOUN, AUX)      (AUX, DET)     (DET, NOUN, AUX)   \n",
       "3           (AUX, DET)      (DET, ADJ)     (NOUN, AUX, DET)   \n",
       "4           (DET, ADJ)     (ADJ, NOUN)      (AUX, DET, ADJ)   \n",
       "5          (ADJ, NOUN)     (NOUN, ADP)     (DET, ADJ, NOUN)   \n",
       "6          (NOUN, ADP)     (ADP, NOUN)     (ADJ, NOUN, ADP)   \n",
       "7          (ADP, NOUN)    (NOUN, VERB)    (NOUN, ADP, NOUN)   \n",
       "8         (NOUN, VERB)     (VERB, ADP)    (ADP, NOUN, VERB)   \n",
       "9          (VERB, ADP)     (ADP, NOUN)    (NOUN, VERB, ADP)   \n",
       "10         (ADP, NOUN)   (NOUN, PUNCT)    (VERB, ADP, NOUN)   \n",
       "11       (NOUN, PUNCT)   (PUNCT, </s>)   (ADP, NOUN, PUNCT)   \n",
       "12         (<s>, NOUN)     (NOUN, AUX)     (<s>, <s>, NOUN)   \n",
       "13         (NOUN, AUX)      (AUX, AUX)     (<s>, NOUN, AUX)   \n",
       "14          (AUX, AUX)     (AUX, VERB)     (NOUN, AUX, AUX)   \n",
       "15         (AUX, VERB)     (VERB, ADP)     (AUX, AUX, VERB)   \n",
       "16         (VERB, ADP)     (ADP, NOUN)     (AUX, VERB, ADP)   \n",
       "17         (ADP, NOUN)   (NOUN, CCONJ)    (VERB, ADP, NOUN)   \n",
       "18       (NOUN, CCONJ)   (CCONJ, NOUN)   (ADP, NOUN, CCONJ)   \n",
       "19       (CCONJ, NOUN)   (NOUN, PUNCT)  (NOUN, CCONJ, NOUN)   \n",
       "\n",
       "        trigram pos next                   Dependents  \\\n",
       "0       (DET, NOUN, AUX)                           []   \n",
       "1       (NOUN, AUX, DET)                          [A]   \n",
       "2        (AUX, DET, ADJ)        [language, system, .]   \n",
       "3       (DET, ADJ, NOUN)                           []   \n",
       "4       (ADJ, NOUN, ADP)                           []   \n",
       "5      (NOUN, ADP, NOUN)          [a, structured, of]   \n",
       "6      (ADP, NOUN, VERB)              [communication]   \n",
       "7      (NOUN, VERB, ADP)                       [used]   \n",
       "8      (VERB, ADP, NOUN)                         [by]   \n",
       "9     (ADP, NOUN, PUNCT)                     [humans]   \n",
       "10   (NOUN, PUNCT, </s>)                           []   \n",
       "11   (PUNCT, </s>, </s>)                           []   \n",
       "12      (NOUN, AUX, AUX)                           []   \n",
       "13      (AUX, AUX, VERB)                           []   \n",
       "14      (AUX, VERB, ADP)                           []   \n",
       "15     (VERB, ADP, NOUN)  [Languages, can, be, on, .]   \n",
       "16    (ADP, NOUN, CCONJ)                     [speech]   \n",
       "17   (NOUN, CCONJ, NOUN)         [and, gesture, sign]   \n",
       "18  (CCONJ, NOUN, PUNCT)                           []   \n",
       "19   (NOUN, PUNCT, VERB)                          [,]   \n",
       "\n",
       "                                         Constituents  \n",
       "0                                                 [A]  \n",
       "1                                       [A, language]  \n",
       "2   [A, language, is, a, structured, system, of, c...  \n",
       "3                                                 [a]  \n",
       "4                                        [structured]  \n",
       "5   [a, structured, system, of, communication, use...  \n",
       "6               [of, communication, used, by, humans]  \n",
       "7                   [communication, used, by, humans]  \n",
       "8                                  [used, by, humans]  \n",
       "9                                        [by, humans]  \n",
       "10                                           [humans]  \n",
       "11                                                [.]  \n",
       "12                                        [Languages]  \n",
       "13                                              [can]  \n",
       "14                                               [be]  \n",
       "15  [Languages, can, be, based, on, speech, and, g...  \n",
       "16  [on, speech, and, gesture, ,, sign, ,, or, wri...  \n",
       "17    [speech, and, gesture, ,, sign, ,, or, writing]  \n",
       "18                                              [and]  \n",
       "19                                       [gesture, ,]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:/Users/desir/Desktop/text_mining/applied TM/nlp technology/test_dependency.conll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-a32f27ff6ea2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;31m# convert pandas df to .conll file (uncomment when ready to use)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[0moutputfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_dependency.conll\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m \u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'c:/Users/desir/Desktop/text_mining/applied TM/nlp technology/{outputfile}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:/Users/desir/Desktop/text_mining/applied TM/nlp technology/test_dependency.conll'"
     ]
    }
   ],
   "source": [
    "with open('test.txt') as test:\n",
    "    sentences = test.readlines()\n",
    "\n",
    "for content in sentences:\n",
    "    content.replace('-', ' ')\n",
    "    print(content)\n",
    "\n",
    "split_content = tokenize.sent_tokenize(content)\n",
    "#print(split_content)\n",
    "tokenized_content = word_tokenize(content)\n",
    "\n",
    "\n",
    "## preprocessing some small things, such as punctuation inbetween a word. Widely-spoken causes the problem that spacy wants to split this, which results in problems when adding it to the dataframe.\n",
    "\n",
    "# create dataframe where new data can be stored:\n",
    "new_df = pd.DataFrame(data=tokenized_content, columns=['Tokens'])\n",
    "\n",
    "## Token bigram extractiong:\n",
    "text = 'test.txt'\n",
    "\n",
    "with open (text, 'r') as infile:\n",
    "    text = infile.readlines()\n",
    "    #print(text)\n",
    "    for sentences in text:\n",
    "        #print(type(sentences))\n",
    "        word_tokenize(sentences)\n",
    "              \n",
    "        tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                          for sent in sent_tokenize(sentences)]\n",
    "              \n",
    "# Set the n-gram size\n",
    "#n = 2\n",
    "\n",
    "# Let us check what is happening on a subset: \n",
    "ngram_data, padded = padded_everygram_pipeline(2, tokenized_text)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "bigram_complete = []\n",
    "#print(\"\\n\\nNGRAMS:\")\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==2 ]\n",
    "    bigram_complete.append(ngram_list)\n",
    "\n",
    "bigram_complete_l = [x[:-1] for x in bigram_complete]    \n",
    "bigram_complete_r = [x[1:] for x in bigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "bigram_token_previous = []\n",
    "for item in bigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_token_previous.append(ele)\n",
    "        \n",
    "bigram_token_next = []\n",
    "for item in bigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_token_next.append(ele)\n",
    "#print(bigram_token)\n",
    "\n",
    "\n",
    "new_df['bigram token previous'] = bigram_token_previous\n",
    "new_df['bigram token next'] = bigram_token_next\n",
    "\n",
    "\n",
    "## Token trigram extraction:\n",
    "\n",
    "text = 'test.txt'\n",
    "with open (text, 'r') as infile:\n",
    "    text = infile.readlines()\n",
    "    #print(text)\n",
    "    for sentences in text:\n",
    "        #print(type(sentences))\n",
    "        word_tokenize(sentences)\n",
    "              \n",
    "        tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "                          for sent in sent_tokenize(sentences)]\n",
    "              \n",
    "# Let us check what is happening on a subset: \n",
    "ngram_data, padded = padded_everygram_pipeline(3, tokenized_text)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "trigram_complete = []\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==3]\n",
    "    trigram_complete.append(ngram_list)\n",
    "\n",
    "    \n",
    "trigram_complete_l = [x[:-2] for x in trigram_complete]   \n",
    "trigram_complete_r = [x[2:] for x in trigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "trigram_token_previous = []\n",
    "for item in trigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_token_previous.append(ele)\n",
    "        \n",
    "trigram_token_next = []\n",
    "for item in trigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_token_next.append(ele)\n",
    "\n",
    "new_df['trigram token previous'] = trigram_token_previous\n",
    "new_df['trigram token next'] = trigram_token_next\n",
    "\n",
    "\n",
    "# Extract dependencies:\n",
    "d_dep = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    dependency_list = []\n",
    "    for item in doc:\n",
    "        dependency_list.append(item.dep_)\n",
    "    d_dep.append(dependency_list)\n",
    "\n",
    "\n",
    "list_dep = []\n",
    "for item in d_dep:\n",
    "    for dep in item:\n",
    "        list_dep.append(dep)\n",
    "\n",
    "## extraction of previous and next bigrams for dependency:\n",
    "\n",
    "ngram_data, padded = padded_everygram_pipeline(2, d_dep)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "bigram_complete = []\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==2 ]\n",
    "    bigram_complete.append(ngram_list)\n",
    "\n",
    "bigram_complete_l = [x[:-1] for x in bigram_complete]    \n",
    "bigram_complete_r = [x[1:] for x in bigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "bigram_dep_previous = []\n",
    "for item in bigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_dep_previous.append(ele)\n",
    "        \n",
    "bigram_dep_next = []\n",
    "for item in bigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_dep_next.append(ele)\n",
    "#print(bigram_token)\n",
    "\n",
    "new_df['Dependency'] = list_dep #--> store dependencies in the new_df dataframe.\n",
    "new_df['bigram dep previous'] = bigram_dep_previous\n",
    "new_df['bigram dep next'] = bigram_dep_next\n",
    "\n",
    "\n",
    "# extraction of previous and next trigrams for dependency:\n",
    "\n",
    "# Let us check what is happening on a subset: \n",
    "ngram_data, padded = padded_everygram_pipeline(3, d_dep)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "trigram_complete = []\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==3]\n",
    "    trigram_complete.append(ngram_list)\n",
    "\n",
    "    \n",
    "trigram_complete_l = [x[:-2] for x in trigram_complete]   \n",
    "trigram_complete_r = [x[2:] for x in trigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "trigram_dep_previous = []\n",
    "for item in trigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_dep_previous.append(ele)\n",
    "        \n",
    "trigram_dep_next = []\n",
    "for item in trigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_dep_next.append(ele)\n",
    "\n",
    "new_df['trigram dep previous'] = trigram_dep_previous\n",
    "new_df['trigram dep next'] = trigram_dep_next\n",
    "\n",
    "# Extract the head word of each node:\n",
    "d = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    dependency_list = []\n",
    "    for item in doc:\n",
    "        dependency_list.append(item.head)\n",
    "    d.append(dependency_list)\n",
    "\n",
    "list_parent = []\n",
    "for item in d:\n",
    "    #print(item)\n",
    "    for parent in item:\n",
    "        list_parent.append(parent)\n",
    "new_df['Direct Parent'] = list_parent\n",
    "\n",
    "\n",
    "#Extract the POS tag of each head of the token, to show in what kind of phrasal structure it is.\n",
    "\n",
    "d = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    dependency_list = []\n",
    "    for item in doc:\n",
    "        dependency_list.append(item.head.pos_)\n",
    "    d.append(dependency_list)\n",
    "\n",
    "list_parent = []\n",
    "for item in d:\n",
    "    #print(item)\n",
    "    for parent in item:\n",
    "        list_parent.append(parent)\n",
    "new_df['POS Parent'] = list_parent\n",
    "\n",
    "\n",
    "## Extracting Pos tag of the token itself:\n",
    "\n",
    "d_pos = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    pos_list = []\n",
    "    for item in doc:\n",
    "        pos_list.append(item.pos_)\n",
    "    d_pos.append(pos_list)\n",
    "\n",
    "list_pos = []\n",
    "for item in d_pos:\n",
    "    #print(item)\n",
    "    for pos in item:\n",
    "        list_pos.append(pos)\n",
    "new_df['POS Token'] = list_pos\n",
    "\n",
    "## extraction of previous and next bigrams for pos tag:\n",
    "\n",
    "ngram_data, padded = padded_everygram_pipeline(2, d_pos)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "bigram_complete = []\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==2 ]\n",
    "    bigram_complete.append(ngram_list)\n",
    "\n",
    "bigram_complete_l = [x[:-1] for x in bigram_complete]    \n",
    "bigram_complete_r = [x[1:] for x in bigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "bigram_pos_previous = []\n",
    "for item in bigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_pos_previous.append(ele)\n",
    "        \n",
    "bigram_pos_next = []\n",
    "for item in bigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        bigram_pos_next.append(ele)\n",
    "#print(bigram_token)\n",
    "new_df['bigram pos previous'] = bigram_pos_previous\n",
    "new_df['bigram pos next'] = bigram_pos_next\n",
    "\n",
    "\n",
    "# extraction of previous and next trigrams for pos:\n",
    "\n",
    "# Let us check what is happening on a subset: \n",
    "ngram_data, padded = padded_everygram_pipeline(3, d_pos)\n",
    "\n",
    "# What is happening during padding? \n",
    "# print(\"PADDING:\")\n",
    "# print(list(padded))\n",
    "\n",
    "# What kind of ngrams do we get? \n",
    "trigram_complete = []\n",
    "for ngrams in ngram_data: \n",
    "    ngram_list = list(ngrams)\n",
    "    ngram_list = [x for x in ngram_list if len(x)==3]\n",
    "    trigram_complete.append(ngram_list)\n",
    "\n",
    "    \n",
    "trigram_complete_l = [x[:-2] for x in trigram_complete]   \n",
    "trigram_complete_r = [x[2:] for x in trigram_complete]\n",
    "\n",
    "#print(bigram_complete)\n",
    "#     print(ngram_list)\n",
    "#     print()\n",
    "    \n",
    "trigram_pos_previous = []\n",
    "for item in trigram_complete_l:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_pos_previous.append(ele)\n",
    "        \n",
    "trigram_pos_next = []\n",
    "for item in trigram_complete_r:\n",
    "    #print(item) \n",
    "    for ele in item:\n",
    "        #print(ele)\n",
    "        trigram_pos_next.append(ele)\n",
    "\n",
    "new_df['trigram pos previous'] = trigram_pos_previous\n",
    "new_df['trigram pos next'] = trigram_pos_next\n",
    "\n",
    "\n",
    "#Extract the children of each token. Will provide an empty list when the token has no child beneath it.\n",
    "d = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    children_list = [list(t.children) for t in doc]\n",
    "    #print(children_list)\n",
    "    d.append(children_list)\n",
    "    \n",
    "list_children = []\n",
    "for item in d:\n",
    "    #print(item)\n",
    "    for child in item:        \n",
    "        list_children.append(child)\n",
    "new_df['Dependents'] = list_children\n",
    "\n",
    "#WE GET THE FULL CONSTITUENTS OF EACH HEAD\n",
    "\n",
    "d = []\n",
    "for word in split_content:\n",
    "    doc = nlp(word)\n",
    "    constituents_list = [list(t.subtree) for t in doc]\n",
    "    d.append(constituents_list)\n",
    "\n",
    "list_constituents = []\n",
    "for item in d:\n",
    "    #print(item)\n",
    "    for constituent in item:        \n",
    "        list_constituents.append(constituent)\n",
    "new_df['Constituents'] = list_constituents\n",
    "\n",
    "\n",
    "display(new_df.head(20))\n",
    "\n",
    "# for word in split_content:\n",
    "#     doc = nlp(word)\n",
    "#     displacy.serve(doc, style='dep')\n",
    "\n",
    "# convert pandas df to .conll file (uncomment when ready to use)\n",
    "outputfile = \"test_dependency.conll\"\n",
    "new_df.to_csv(f'c:/Users/desir/Desktop/text_mining/applied TM/nlp technology/{outputfile}', sep='\\t', header=True, quotechar='|', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d04147-e082-4db0-bb4e-ac387db0a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# new_d = []\n",
    "# for sentences in split_content:\n",
    "#     #print(sentences)\n",
    "#     doc_stanford= nlp_stanford(sentences)\n",
    "#     #print(doc_stanford)\n",
    "#     new_d.append(doc_stanford)\n",
    "# #print(new_d)\n",
    "\n",
    "\n",
    "\n",
    "#for word in new_d:\n",
    "   # print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e757e193-d56d-4c6d-b753-1c8735aeeef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['a', 'language', 'is', 'a', 'structured', 'system', 'of', 'communication', 'used', 'by', 'humans', '.']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-0aa1110ba55e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;31m#call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \"\"\"\n\u001b[1;32m--> 988\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    989\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    990\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1068\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             )\n\u001b[1;32m-> 1070\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m     def update(\n",
      "\u001b[1;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "# text = 'test.txt'\n",
    "# with open (text, 'r') as infile:\n",
    "#     text = infile.readlines()\n",
    "#     #print(text)\n",
    "#     for sentences in text:\n",
    "#         #print(type(sentences))\n",
    "#         #word_tokenize(sentences)\n",
    "              \n",
    "#         tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "#                           for sent in sent_tokenize(sentences)]\n",
    "\n",
    "# print(type(tokenized_text))\n",
    "\n",
    "# d = []\n",
    "# for sentence in tokenized_text:\n",
    "#     print(sentence)\n",
    "#     doc=nlp(sentence)\n",
    "#     for word in sentence:\n",
    "#         print(word)\n",
    "#         doc = nlp(word)\n",
    "#         dependency_list=[]\n",
    "#         for item in doc:\n",
    "#             print(item)\n",
    "#             dependency = item.dep_\n",
    "#             print(dependency)\n",
    "#             dependency_list.append(dependency)\n",
    "#             print(dependency_list)\n",
    "#     d.append(dependency_list)\n",
    "# print(d)\n",
    "\n",
    "\n",
    "# list_dep = []\n",
    "# for item in d:\n",
    "#     print(item)\n",
    "#     for dep in item:\n",
    "#         list_dep.append(dep)\n",
    "\n",
    "# # Let us check what is happening on a subset: \n",
    "# ngram_data, padded = padded_everygram_pipeline(3, list_dep)\n",
    "\n",
    "# # What is happening during padding? \n",
    "# # print(\"PADDING:\")\n",
    "# # print(list(padded))\n",
    "\n",
    "# # What kind of ngrams do we get? \n",
    "# trigram_complete = []\n",
    "# print(\"\\n\\nNGRAMS:\")\n",
    "# for ngrams in ngram_data: \n",
    "#     ngram_list = list(ngrams)\n",
    "#     ngram_list = [x for x in ngram_list if len(x)==3]\n",
    "#     trigram_complete.append(ngram_list)\n",
    "    \n",
    "# trigram_complete_l = [x[:-2] for x in trigram_complete]   \n",
    "# trigram_complete_r = [x[2:] for x in trigram_complete]\n",
    "\n",
    "# #print(bigram_complete)\n",
    "# #     print(ngram_list)\n",
    "# #     print()\n",
    "    \n",
    "# trigram_token_previous = []\n",
    "# for item in trigram_complete_l:\n",
    "#     #print(item) \n",
    "#     for ele in item:\n",
    "#         #print(ele)\n",
    "#         trigram_token_previous.append(ele)\n",
    "        \n",
    "# trigram_token_next = []\n",
    "# for item in trigram_complete_r:\n",
    "#     #print(item) \n",
    "#     for ele in item:\n",
    "#         #print(ele)\n",
    "#         trigram_token_next.append(ele)\n",
    "\n",
    "# new_df['trigram token previous'] = trigram_token_previous\n",
    "# new_df['trigram token next'] = trigram_token_next\n",
    "\n",
    "# display(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b583bb9a-7a49-4ce2-ab06-73ebdc789a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
